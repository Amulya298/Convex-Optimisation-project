# -*- coding: utf-8 -*-
"""ee21mtech12003_hw1_exercise_1.7.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1oNxPMH8Jje9lgJuPAifm_ipeOmYh_7sl

Exercise 1.7. For each of the following functions, find the minimizer by setting the gradient to zero, and
also using gradient descent (in python) with T “ 1000 and δ “ 0.02. You can use np.random.rand to select
x0
"""

import numpy as np
x_01 = np.random.rand((3))
x_01=np.reshape(x_01,(3,1))
x_02 = np.random.rand((2))
x_02=np.reshape(x_02,(2,1))

def derivative_f(v,A,b,func):
  if (func==1):
    return np.matmul((A+A.T),v)
  elif (func==2 or func==3):
    t1=np.matmul(A.T,A)
    return 2*(np.matmul(t1,v)-np.matmul(A.T,b))

def optimum_x(x_0,A,b,func,T,d):
  
  x=np.zeros(x_0.shape,float)
  for i in range(T):
    #print("current_x:",i,x)
    if i==0:
      x = x_0
    else:
      x = x-d*derivative_f(x,A,b,func)
    #print("updated_x:",i,x)
  return x

A1=np.array([[2,-1,-1],[-1,2,0],[-1,0,1]])
b1=np.array([[1]])

A2=np.array([[1,2],[2,4],[3,1]])
b2=np.array([[1,3,1]]).T

A3=np.array([[1,2,1],[2,4,2],[3,1,9],[4,1,0],[2,1,4]])
b3=np.array([[1,3,1,0,9]]).T

x_1=optimum_x(x_01,A1,b1,1,1000,0.02)
print("optimum x for ques 1.7.1\n",x_1)

x_2=optimum_x(x_02,A2,b2,2,1000,0.02)
print("optimum x for ques 1.7.2\n",x_2)

x_3=optimum_x(x_01,A3,b3,3,1000,0.002)
print("optimum x for ques 1.7.3\n",x_3)

### all the optimum x s of exersice 1.7, found through theoritical calculations matches with the above gradient descent code