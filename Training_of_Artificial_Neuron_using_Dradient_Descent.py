# -*- coding: utf-8 -*-
"""EE21MTECH12003_HW1_Exercise_1.8.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hY_qvXVuHQAHp4DTWIJ1fp93jMFxZrgc
"""

import matplotlib.pyplot as plt
import numpy as np

def g_f_activation(xi):
  act= np.exp(xi)/(1+np.exp(xi))
  return act

def derv_g_f(xi):
  act_der= np.exp(xi)/((1+np.exp(xi))**2)
  return act_der

def gradient_f(X_,w_,y_):
  t1=np.matmul(X_.T,w_)
  t2=g_f_activation((t1))
  t2=np.reshape(t2,(len(t2),1))
  t3=t2-y_
  t4= derv_g_f(t1)*t3
  t5=2*np.matmul(X_,t4)
  return t5

def optimum_w(X,w_0,y,T,d):
  los=[]
  w=np.zeros(w_0.shape,float)
  for i in range(T):
    if i==0:
      w = w_0
    else:
      w = w-d*gradient_f(X,w,y)
    loss_= loss_fun(X, w, y)
    los.append(loss_)
  print("loss at 100th step with leraning rate 0.05: ",loss_)
  print("optimal weight vector w: \n ",w)
  # los=np.array(los)
  # ls=np.arange(0,100)
  # print((los).shape,(ls).shape)
  # plt.figure()
  # plt.plot(ls,los)
  return w

def y_label(x):
  y= np.zeros(x.shape[1],int)
  for i in range(x.shape[1]):
    #print(x.shape[1],x[:,i],x[0,i],x[1,i],x[3,i])
    if (2*x[0,i]-3*x[1,i]+x[3,i])>0:
      y[i]=1
    else:
      y[i]=0
  return y

def loss_fun(Xi, w_opt, y_gt):
  y_pred = np.zeros(y_gt.shape,float)
  y_pred = g_f_activation(np.matmul( Xi.T,w_opt))
  y_gt=np.reshape(y_gt,(len(y_gt),1))
  return ((y_pred - y_gt) ** 2).mean()


def y_label(x):
  y= np.zeros(x.shape[1],int)
  for i in range(x.shape[1]):
    if (2*x[0,i]-3*x[1,i]+x[3,i])>0:
      y[i]=1
    else:
      y[i]=0
    #print("check",y[i])
  return y

#####plot of activation function exp(x)/(1+exp(x))
l=np.arange(-15, 15, 0.05)
plt.figure()
plt.plot(l,g_f_activation(l))
plt.title("Activation function")

import numpy as np
xr= np.random.rand(4,100)
w_0=np.random.rand(4,1)
y_g=y_label(xr)
y_g=np.reshape(y_g,(len(y_g),1))

w_op=optimum_w(xr,w_0,y_g,100,0.05)