# -*- coding: utf-8 -*-
"""multi class svm using hinge loss on_iris .ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yv4D05J35WwyVFnvMO-WYGh7gbrTojsL
"""

# Importing libraries here. Note: The straight forward libraries are used in situations where the main part of 
# analysis isnt of much importance. 
import numpy as np # for array operations
import cvxpy as cp # for convex problem related operations. 
import matplotlib.pyplot as plt # for plotting purposes. 
# The Sklearn library in python is used to load data and split it to training and testing data. 
from sklearn.datasets import load_iris 
from sklearn.model_selection import train_test_split
from tqdm import tqdm
# define a function for the confusion matrix.
def conf_matr(n_classes,y_pred,y_test):
  cm = np.zeros((n_classes,n_classes)) # an empty array 
  for true_label, pred_label in zip(y_test, y_pred): # for the particular GT and the predicted label. 
    cm[true_label,pred_label] += 1 # increment at those locations. 
  return cm

# define accuracy function here. 
def acc(y_test,y_pred):
  ct = np.sum(y_test==y_pred) # the sum of correctly predicted labels.
  accr = ct/len(y_test) # divide the above with the length of the test labels. 
  return accr

def loss_fun(w,x,y,l):
  
  '''
  hinge_loss=0
  for i in range(len(y)):
    yi=y[i]

    for j in range(10):
      if (j!=yi):
        v1=w[j,:]
        v1=v1.reshape((v1.shape[0],1))
        v2=x[i]
        v2=v2[np.newaxis, :]

        v3=w[yi,:]
        v3=v3.reshape((v1.shape[0],1))
        
        hinge_loss=hinge_loss+np.maximum(0,(np.matmul(v2,v1)-np.matmul(v2,v3)+1).item())
    
    
  term2=np.sum(np.square(w))
  total_loss=hinge_loss+l*term2
  '''
  ##accuracy
 
  scores=x@w.T
  print(scores.shape)
  y_pred=np.argmax(scores,axis=1)
  y_pred=y_pred.reshape((len(y_pred),1))
  c=0
  ct = np.sum(y==y_pred) # the sum of correctly predicted labels.
  accr = ct/len(y)
  loss=1-accr
  print(len(y),y_pred,y)
  #",loss,accuracy)
  return accr,loss,y_pred

def cal_grad_t2(x1,y1,w1,delta):
  t2=0
  
  for i in range(len(x1)):
    yi=y1[i]
    for j in range(3):
    
      if w1[j,:]@x1[i]-w1[yi,:]@x1[i]+delta>0 and yi!=j:
        t2+=x1[i]

  return t2


def optim_W(xtr,ytr,w_op,lr1,lr2,delta):
  for i in range(len(xtr)):
    yi=ytr[i]
    gd_t2=cal_grad_t2(xtr,ytr,w_op,delta)
    for j in range(3):
      if yi==j:
        w_op[yi,:]=(1-lr1)*w_op[yi,:]+gd_t2
      else:
        w_op[j,:]=(1-lr2)*w_op[j,:]-gd_t2
  return w_op

def loss_acc(xtr,ytr,it,lr1,lr2,lamb,delta):
  w = np.random.rand(3,len(xtr[0])+1)
  N=len(xtr)
  ones=np.ones((N,1))
  x_new=np.hstack((xtr,ones))
  loss_f=[]
  acc=[]
  for i in tqdm(range(it)):
    if i==0:
      w_opt=w
    else:
      w_opt=optim_W(x_new,ytr,w_opt,lr1,lr2,delta)
    accu,loss,y2=loss_fun(w_opt,x_new,ytr,lamb)
   
    loss_f.append(loss)
    acc.append(accu)
  
  plt.figure()

  plt.title('loss using hinge loss function')
  plt.plot(np.asarray(loss_f))
  plt.figure()
  plt.title('accuracy using hinge loss function')
  plt.plot(np.asarray(acc))
  print("loss**********",np.asarray(loss_f))
  print("acc**********",np.asarray(acc))
  return w_opt

iris = load_iris() # load the dataset here. 
X = iris.data # the input that contains the height and the width of the petal and sepal respectively. 
y =  iris.target # this contains the labels here. 

# Split data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1) # split the dataset here. 
# Train and test using One-vs-One approach
w_opti = loss_acc(X_train,y_train,10,0.0002,0.0001,0.1,1)
'''#(, , X_test, y_test)
# this finds out the predicted label. 
cm = conf_matr(len(np.unique(y_test))) # find the confusion matrix; a function is created here. (sklearn can also be used for verification.)
ctp = np.sum(y_pred==y_test) # the count of correctly predicted labels. 
accuracy =  ctp/len(y_pred) # the number of correctly predicted labels by the total length of the prediction. 
print('Accuracy:', accuracy)

plt.figure()
plt.imshow(cm) # plot the confusion matrix. 
plt.colorbar()
'''
N=len(X_test)
ones=np.ones((N,1))
x_test=np.hstack((X_test,ones))

def conf_matr(n_classes,y_pred,y_test):
  cm = np.zeros((n_classes,n_classes)) # an empty array 
  for true_label, pred_label in zip(y_test, y_pred): # for the particular GT and the predicted label. 
    cm[true_label,pred_label] += 1 # increment at those locations. 
  return cm
acc,loss,yp=loss_fun(w_opti,x_test,y_test,1)
con=conf_matr(3,yp,y_test)
plt.figure()
plt.title('Multi-class SVM using hinge loss ')
plt.imshow(con)
print(acc,loss)

def conf_matr(n_classes,y_pred,y_test):
  cm = np.zeros((n_classes,n_classes)) # an empty array 
  for true_label, pred_label in zip(y_test, y_pred): # for the particular GT and the predicted label. 
    cm[true_label,pred_label] += 1 # increment at those locations. 
  return cm
acc,loss,yp=loss_fun(wt,x_test,y_test)
con=conf_matr(10,yp,y_test)
plt.figure()
plt.title('Multi-class SVM using hinge loss ')
plt.imshow(con)