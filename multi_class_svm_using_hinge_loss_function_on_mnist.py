# -*- coding: utf-8 -*-
"""Multi class SVM using hinge loss function on MNIST.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1W626h0WZEjglTf6iQHuxgFNZ81yHgtR2
"""

import numpy as np
import matplotlib.pyplot as plt
import cvxpy as cp
from numpy import linalg
import csv 
from tqdm import tqdm

#Mount the drive
from google.colab import drive
from mlxtend.data import loadlocal_mnist
import matplotlib.pyplot as plt

drive.mount('/content/drive',force_remount=True)
root_path='/content/drive/MyDrive/MNIST'
train_data_path = str(root_path) + "/" + 'train-images.idx3-ubyte'
train_label_path = str(root_path) + "/" + 'train-labels.idx1-ubyte'
test_data_path = str(root_path) + "/" + 't10k-images.idx3-ubyte'
test_label_path = str(root_path) + "/" + 't10k-labels.idx1-ubyte'

X_train, Y_train = loadlocal_mnist(images_path=train_data_path, labels_path=train_label_path)
X_test, Y_test = loadlocal_mnist(images_path=test_data_path, labels_path=test_label_path)

no_tr=500
no_tst=100
x_train = X_train[:no_tr,:]
y_train = Y_train[:no_tr]
x_test = X_test[:no_tst,:]
y_test = Y_test[:no_tst]
y_train=y_train.reshape((no_tr,1))
y_test=y_test.reshape((no_tst,1))

def loss_fun(w,x,y,l):
  
  '''
  hinge_loss=0
  for i in range(len(y)):
    yi=y[i]

    for j in range(10):
      if (j!=yi):
        v1=w[j,:]
        v1=v1.reshape((v1.shape[0],1))
        v2=x[i]
        v2=v2[np.newaxis, :]

        v3=w[yi,:]
        v3=v3.reshape((v1.shape[0],1))
        
        hinge_loss=hinge_loss+np.maximum(0,(np.matmul(v2,v1)-np.matmul(v2,v3)+1).item())
    
    
  term2=np.sum(np.square(w))
  total_loss=hinge_loss+l*term2
  '''
  ##accuracy
 
  scores=x@w.T
  print(scores.shape)
  y_pred=np.argmax(scores,axis=1)
  y_pred=y_pred.reshape((len(y_pred),1))
  c=0
  for i in range(10):
    if y_pred[i]!=y[i]:
      c=c+1 
  loss=c/len(y)
  accuracy=1-loss
  #print("############",loss,accuracy)
  return accuracy,loss,y_pred

def cal_grad_t2(x1,y1,w1,delta):
  t2=0
  
  for i in range(len(x1)):
    yi=y1[i]
    for j in range(10):
    
      if w1[j,:]@x1[i]-w1[yi,:]@x1[i]+delta>0 and yi!=j:
        t2+=x1[i]

  return t2


def optim_W(xtr,ytr,w_op,lr1,lr2,delta):
  for i in range(len(xtr)):
    yi=ytr[i]
    gd_t2=cal_grad_t2(xtr,ytr,w_op,delta)
    for j in range(10):
      if yi==j:
        w_op[yi,:]=(1-lr1)*w_op[yi,:]+gd_t2
      else:
        w_op[j,:]=(1-lr2)*w_op[j,:]-gd_t2
  return w_op

def loss_acc(xtr,ytr,it,lr1,lr2,lamb,delta):
  w = np.random.rand(10,len(xtr[0])+1)
  N=len(xtr)
  ones=np.ones((N,1))
  x_new=np.hstack((xtr,ones))
  loss_f=[]
  acc=[]
  for i in tqdm(range(it)):
    if i==0:
      w_opt=w
    else:
      w_opt=optim_W(x_new,ytr,w_opt,lr1,lr2,delta)
    accu,loss,y1=loss_fun(w_opt,x_new,ytr,lamb)
   
    loss_f.append(loss)
    acc.append(accu)
  
  plt.figure()
  plt.subplot(1,2,1)
  plt.title('loss')
  plt.plot(np.asarray(loss_f))
  plt.subplot(1,2,2)
  plt.title('accuracy')
  plt.plot(np.asarray(acc))
  print("loss**********",np.asarray(loss_f))
  print("acc**********",np.asarray(acc))
  return w_opt
wt=loss_acc(x_train,y_train,50,0.01,0.01,1,5)

N=len(x_test)
ones=np.ones((N,1))
x_test=np.hstack((x_test,ones))
acc,loss=loss_fun(wt,x_test,y_test,1)
print("@@@@@",acc,loss)

def conf_matr(n_classes,y_pred,y_test):
  cm = np.zeros((n_classes,n_classes)) # an empty array 
  for true_label, pred_label in zip(y_test, y_pred): # for the particular GT and the predicted label. 
    cm[true_label,pred_label] += 1 # increment at those locations. 
  return cm
acc,loss,yp=loss_fun(wt,x_test,y_test)
con=conf_matr(10,yp,y_test)
plt.figure()
plt.title('Multi-class SVM using hinge loss ')
plt.imshow(con)